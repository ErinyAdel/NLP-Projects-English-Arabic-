{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from string import punctuation\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167516"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load/Scrap Text From This Link\n",
    "content = requests.get(\"http://www.gutenberg.org/cache/epub/11/pg11.txt\").text\n",
    "FILE_PATH = \"wonderland.txt\"\n",
    "open(FILE_PATH, \"w\", encoding=\"utf-8\").write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 50\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 30\n",
    "BASENAME = os.path.basename(FILE_PATH)\n",
    "\n",
    "### Read The Data\n",
    "text = open(FILE_PATH, encoding=\"utf-8\").read()\n",
    "\n",
    "### Clean The Data\n",
    "## Remove caps\n",
    "text = text.lower()\n",
    "## Remove punctuation\n",
    "text = text.translate(str.maketrans(\"\", \"\", punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters:\n",
      " 0123456789abcdefghijklmnopqrstuvwxyz﻿\n",
      "Number of characters: 158596\n",
      "Number of unique characters: 39\n"
     ]
    }
   ],
   "source": [
    "## Print Some Stats\n",
    "n_chars = len(text)\n",
    "vocab = ''.join(sorted(set(text))) ## Join All Text Without Spaces --> Take It As Set Of Letters Not Words\n",
    "n_unique_chars = len(vocab)\n",
    "\n",
    "print(\"Unique characters:\", vocab)\n",
    "print(\"Number of characters:\", n_chars)\n",
    "print(\"Number of unique characters:\", n_unique_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\n', ' ')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[0], vocab[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '0': 2, '1': 3, '2': 4, '3': 5, '4': 6, '5': 7, '6': 8, '7': 9, '8': 10, '9': 11, 'a': 12, 'b': 13, 'c': 14, 'd': 15, 'e': 16, 'f': 17, 'g': 18, 'h': 19, 'i': 20, 'j': 21, 'k': 22, 'l': 23, 'm': 24, 'n': 25, 'o': 26, 'p': 27, 'q': 28, 'r': 29, 's': 30, 't': 31, 'u': 32, 'v': 33, 'w': 34, 'x': 35, 'y': 36, 'z': 37, '\\ufeff': 38} \n",
      "\n",
      "{0: '\\n', 1: ' ', 2: '0', 3: '1', 4: '2', 5: '3', 6: '4', 7: '5', 8: '6', 9: '7', 10: '8', 11: '9', 12: 'a', 13: 'b', 14: 'c', 15: 'd', 16: 'e', 17: 'f', 18: 'g', 19: 'h', 20: 'i', 21: 'j', 22: 'k', 23: 'l', 24: 'm', 25: 'n', 26: 'o', 27: 'p', 28: 'q', 29: 'r', 30: 's', 31: 't', 32: 'u', 33: 'v', 34: 'w', 35: 'x', 36: 'y', 37: 'z', 38: '\\ufeff'}\n"
     ]
    }
   ],
   "source": [
    "## Dictionary its Keys: Characters, Values: Integers(indeices)\n",
    "char2int = {c: i for i, c in enumerate(vocab)}\n",
    "print(char2int, '\\n')\n",
    "\n",
    "## Dictionary its Keys: Integers(indeices), Values: Characters\n",
    "int2char = {i: c for i, c in enumerate(vocab)}\n",
    "print(int2char)\n",
    "\n",
    "## Save these dictionaries for later generation\n",
    "pickle.dump(char2int, open(f\"{BASENAME}-char2int.pickle\", \"wb\"))\n",
    "pickle.dump(int2char, open(f\"{BASENAME}-int2char.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38 27 29 ... 30  0  0] \n",
      "\n",
      "<TensorSliceDataset shapes: (), types: tf.int32>\n"
     ]
    }
   ],
   "source": [
    "## Convert all text into integers\n",
    "encoded_text = np.array([char2int[c] for c in text])\n",
    "print(encoded_text, '\\n')\n",
    "    \n",
    "## Convert all integers to Tensor --> Construct Tensor Object\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
    "print(char_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 ﻿\n",
      "27 p\n",
      "29 r\n",
      "26 o\n",
      "21 j\n",
      "16 e\n",
      "14 c\n",
      "31 t\n"
     ]
    }
   ],
   "source": [
    "## Print first 8 characters (Included white-space)\n",
    "for char in char_dataset.take(8):\n",
    "    print(char.numpy(), int2char[char.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (201,), types: tf.int32> \n",
      "____________________________________________________\n",
      "﻿project gutenbergs alices adventures in wonderland by lewis carroll\n",
      "\n",
      "\n",
      "\n",
      "this ebook is for the use of anyone anywhere at no cost and with\n",
      "\n",
      "almost no restrictions whatsoever  you may copy it give it away\n",
      " or\n",
      "\n",
      "reuse it under the terms of the project gutenberg license included\n",
      "\n",
      "with this ebook or online at wwwgutenbergorg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "title alices adventures in wonderland\n",
      "\n",
      "\n",
      "\n",
      "author lewis carroll\n",
      "\n",
      "\n",
      "\n",
      "posting date \n"
     ]
    }
   ],
   "source": [
    "## Build Sequences By Batching\n",
    "sequences = char_dataset.batch(2*sequence_length + 1, drop_remainder=True)\n",
    "print(sequences, '\\n____________________________________________________\\n')\n",
    "    \n",
    "## Print Sequences\n",
    "for sequence in sequences.take(2):\n",
    "    print(''.join([int2char[i] for i in sequence.numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "789\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<FlatMapDataset shapes: ((50,), ()), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Prepare inputs (x) and targets (y)\n",
    "def split_sample(sample): ##                          X        ,               Y\n",
    "    ds = tf.data.Dataset.from_tensors((sample[:sequence_length], sample[sequence_length]))\n",
    "    for i in range(1, (len(sample)-1) // 2): ## range(1, 394) --> (789 - 1) // 2\n",
    "        \n",
    "        input_ = sample[i: i+sequence_length]\n",
    "        target = sample[i+sequence_length]\n",
    "\n",
    "        ## Extend The Dataset with these samples by concatenate() method\n",
    "        other_ds = tf.data.Dataset.from_tensors((input_, target))\n",
    "        ds = ds.concatenate(other_ds)\n",
    "    return ds\n",
    "\n",
    "\n",
    "print(len(sequences))\n",
    "dataset = sequences.flat_map(split_sample)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((50, 39), (39,)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def one_hot_samples(input_, target):\n",
    "    # one-hot encode the inputs and the targets\n",
    "    return tf.one_hot(input_, n_unique_chars), tf.one_hot(target, n_unique_chars)\n",
    "\n",
    "\n",
    "dataset = dataset.map(one_hot_samples)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Input: ﻿project gutenbergs alices adventures in wonderlan\n",
      "Target: d\n",
      "Input shape: (50, 39)\n",
      "Target shape: (39,)\n",
      "================================================================================\n",
      "Input: project gutenbergs alices adventures in wonderland\n",
      "Target:  \n",
      "Input shape: (50, 39)\n",
      "Target shape: (39,)\n",
      "================================================================================\n",
      "Input: roject gutenbergs alices adventures in wonderland \n",
      "Target: b\n",
      "Input shape: (50, 39)\n",
      "Target shape: (39,)\n",
      "================================================================================\n",
      "Input: oject gutenbergs alices adventures in wonderland b\n",
      "Target: y\n",
      "Input shape: (50, 39)\n",
      "Target shape: (39,)\n",
      "================================================================================\n",
      "Input: ject gutenbergs alices adventures in wonderland by\n",
      "Target:  \n",
      "Input shape: (50, 39)\n",
      "Target shape: (39,)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "## Print First 2 Samples\n",
    "print(\"=\"*80)\n",
    "for element in dataset.take(5):\n",
    "    print(\"Input:\", ''.join([int2char[np.argmax(char_vector)] for char_vector in element[0].numpy()]))\n",
    "    print(\"Target:\", int2char[np.argmax(element[1].numpy())])\n",
    "    print(\"Input shape:\", element[0].shape)\n",
    "    print(\"Target shape:\", element[1].shape)\n",
    "    \n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Repeat, Shuffle and Batch The Dataset\n",
    "ds = dataset.repeat().shuffle(1024).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build The Model\n",
    "model = Sequential([\n",
    "    LSTM(256, input_shape=(sequence_length, n_unique_chars), return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(256),\n",
    "    Dense(n_unique_chars, activation=\"softmax\"),## The Output Layer --> No. Of Classes: n_unique_chars, Activation: softmax\n",
    "])\n",
    "\n",
    "## Train the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(ds, steps_per_epoch=(len(encoded_text) - sequence_length) // BATCH_SIZE, epochs=1)\n",
    "\n",
    "## Make \"results folder\" if does not exist yet\n",
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "\n",
    "## Save the model\n",
    "model.save(f\"results/{BASENAME}-{sequence_length}.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
